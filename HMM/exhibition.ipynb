{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Hidden Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " *  Experimenting with hidden Markov models, in particular, applying them to modeling character sequences, and analyzing the learned solution.\n",
    " * `hmm.py` contains a basic implementation of an HMM and of the Baum-Welch training algorithm.\n",
    " * The names of variables used in the code and the references to equations are taken from the HMM paper by Rabiner et al.\n",
    " * In addition to the variables described in this paper, we use two additional variables: $Z$ for the emission probabilities of observations $O$, and $\\psi$ (i.e. psi) for collecting the statistics of Equation (40c)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hmm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of a Small HMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first look at a toy example of an HMM trained on a binary sequence. The training procedure below consists of 100 iterations of the Baum-Welch procedure. It runs the HMM learning algorithm for some toy binary data and prints the parameters learned by the HMM (i.e. matrices $A$ and $B$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "0.000 1.000 0.000 0.000\n",
      "0.000 0.000 1.000 0.000\n",
      "0.000 0.000 0.000 1.000\n",
      "1.000 0.000 0.000 0.000\n",
      " \n",
      "B\n",
      "0.800 0.200\n",
      "0.880 0.120\n",
      "0.000 1.000\n",
      "0.720 0.280\n",
      " \n",
      "Pi\n",
      "0.000\n",
      "0.000\n",
      "1.000\n",
      "0.000\n"
     ]
    }
   ],
   "source": [
    "O = np.array([1,0,1,0,1,1,0,0,1,0,0,0,1,1,1,0,1,0,0,0,1,1,0,1,1,0,0,1,1,\n",
    "                 0,0,0,1,0,0,0,1,1,0,0,1,0,0,1,1,0,0,0,1,0,1,0,1,0,0,0,1,0,\n",
    "                 0,0,1,0,1,0,1,0,0,0,1,1,1,0,1,0,0,0,1,0,0,0,1,0,1,0,1,0,0,\n",
    "                 0,1,1,1,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,1,0,0,1,0,1,1,\n",
    "                 1,0,0,0,1,1,0,0,1,0,1,1,1,0,0,1,1,0,0,0,1,1,0,0,1,1,0,0,1,\n",
    "                 0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,1,0,1,0,0,0,1,0,0,0,1,0,\n",
    "                 0,0,1,0,0,0,1,1,0,0,1,1,0,0,1,1,0,0,1,0,0,0,1,1,0,0])\n",
    "\n",
    "hmmtoy = hmm.HMM(4,2)\n",
    "\n",
    "for k in range(100):\n",
    "    hmmtoy.loaddata(O)\n",
    "    hmmtoy.forward()\n",
    "    hmmtoy.backward()\n",
    "    hmmtoy.learn()\n",
    "\n",
    "print('A')\n",
    "print(\"\\n\".join([\" \".join(['%.3f'%a for a in aa]) for aa in hmmtoy.A]))\n",
    "print(' ')\n",
    "print('B')\n",
    "print(\"\\n\".join([\" \".join(['%.3f'%b for b in bb]) for bb in hmmtoy.B]))\n",
    "print(' ')\n",
    "print('Pi')\n",
    "print(\"\\n\".join(['%.3f'%b for b in hmmtoy.Pi]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the best number $N$ of hidden states\n",
    "\n",
    "For the same sequence of observations as before, we would like to determine automatically what is a good number of hidden states $N = \\mathrm{card}(S)$ for the model. Procedure:\n",
    "\n",
    "* *Split* the sequence of observations into a training and test set (assuming stationarity).\n",
    "* *Train* the model on the training set for several iteration (e.g. 100 iterations) and for multiple parameter $N$.\n",
    "* *Display* for each choice of parameter $N$ the log-probability $\\log p(O | \\lambda)$ for the test set. (If the results are unstable, perform several trials of the same experiment for each parameter $N$.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 2\n",
      "logptrain: -56.24, logptest-61.57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logptrain: -67.58, logptest-66.83\n",
      "logptrain: -56.24, logptest-61.57\n",
      "logptrain: -56.24, logptest-61.57\n",
      "N = 3\n",
      "logptrain: -55.80, logptest-61.01\n",
      "logptrain: -56.24, logptest-61.57\n",
      "logptrain: -60.72, logptest-60.86\n",
      "logptrain: -55.80, logptest-61.02\n",
      "N = 4\n",
      "logptrain: -37.77, logptest-36.30\n",
      "logptrain: -60.03, logptest-63.07\n",
      "logptrain: -37.77, logptest-36.30\n",
      "logptrain: -37.77, logptest-36.30\n",
      "N = 5\n",
      "logptrain: -55.11, logptest-61.94\n",
      "logptrain: -37.77, logptest-36.30\n",
      "logptrain: -37.77, logptest-36.30\n",
      "logptrain: -37.77, logptest-36.30\n",
      "N = 6\n",
      "logptrain: -37.77, logptest-36.29\n",
      "logptrain: -36.71, logptest-52.61\n",
      "logptrain: -53.77, logptest-60.25\n",
      "logptrain: -37.77, logptest-36.30\n",
      "N = 7\n",
      "logptrain: -36.05, logptest-41.35\n",
      "logptrain: -51.23, logptest-90.27\n",
      "logptrain: -52.84, logptest-62.42\n",
      "logptrain: -36.71, logptest-63.49\n",
      "N = 8\n",
      "logptrain: -36.71, logptest-57.10\n",
      "logptrain: -36.89, logptest-38.78\n",
      "logptrain: -36.89, logptest-61.87\n",
      "logptrain: -32.39, logptest-95.28\n",
      "N = 9\n",
      "logptrain: -32.06, logptest-138.28\n",
      "logptrain: -32.45, logptest-74.37\n",
      "logptrain: -32.04, logptest-130.47\n",
      "logptrain: -32.37, logptest-117.67\n",
      "N = 10\n",
      "logptrain: -30.55, logptest-57.90\n",
      "logptrain: -30.56, logptest-109.79\n",
      "logptrain: -32.77, logptest-113.43\n",
      "logptrain: -34.94, logptest-60.23\n",
      "N = 11\n",
      "logptrain: -31.81, logptest-39.86\n",
      "logptrain: -32.11, logptest-109.39\n",
      "logptrain: -31.69, logptest-100.05\n",
      "logptrain: -37.77, logptest-36.30\n",
      "N = 12\n",
      "logptrain: -30.55, logptest-109.55\n",
      "logptrain: -28.38, logptest-172.69\n",
      "logptrain: -33.79, logptest-57.86\n",
      "logptrain: -34.94, logptest-45.63\n",
      "N = 13\n",
      "logptrain: -30.46, logptest-67.72\n",
      "logptrain: -31.39, logptest-136.45\n",
      "logptrain: -29.48, logptest-188.94\n",
      "logptrain: -27.95, logptest-inf\n",
      "N = 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1788/3228040480.py:22: RuntimeWarning: divide by zero encountered in log\n",
      "  logptest = np.log(hmmtoy.pobs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logptrain: -31.55, logptest-98.70\n",
      "logptrain: -29.58, logptest-103.03\n",
      "logptrain: -33.48, logptest-95.11\n",
      "logptrain: -32.33, logptest-96.16\n",
      "N = 15\n",
      "logptrain: -27.88, logptest-529.22\n",
      "logptrain: -33.52, logptest-394.26\n",
      "logptrain: -31.81, logptest-39.86\n",
      "logptrain: -27.65, logptest-179.63\n"
     ]
    }
   ],
   "source": [
    "for N in range(2,16):\n",
    "    print(f'N = {N}')\n",
    "    \n",
    "    for i in range(4):\n",
    "        hmmtoy = hmm.HMM(N,2)\n",
    "        Otrain = O[:len(O)//2]\n",
    "        Otest = O[len(O)//2:]\n",
    "\n",
    "        for k in range(100):\n",
    "\n",
    "            hmmtoy.loaddata(Otrain)\n",
    "            hmmtoy.forward()\n",
    "            hmmtoy.backward()\n",
    "            hmmtoy.learn()\n",
    "\n",
    "        hmmtoy.loaddata(Otrain)\n",
    "        hmmtoy.forward()\n",
    "        logptrain = np.log(hmmtoy.pobs)\n",
    "\n",
    "        hmmtoy.loaddata(Otest)\n",
    "        hmmtoy.forward()\n",
    "        logptest = np.log(hmmtoy.pobs)\n",
    "\n",
    "        print(f'logptrain: {logptrain:.2f}, logptest{logptest:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text modeling and generation\n",
    "\n",
    "Now we will train a HMM on character sequences taken from English text. We use the 20 newsgroups dataset that is accessible via scikits-learn http://scikit-learn.org/stable/datasets/twenty_newsgroups.html. Documentation is available on the website. The code below allows you to (1) read the dataset, (2) sample HMM-readable sequences from it, and (3) convert them back into string of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "# Download a subset of the newsgroup dataset\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',categories=['sci.med'])\n",
    "newsgroups_test  = fetch_20newsgroups(subset='test' ,categories=['sci.med'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a sequence of T characters from the dataset that the HMM can\n",
    "# read (0=whitespace 1-26=A-Z).\n",
    "def sample(data,T=50):\n",
    "    i = np.random.randint(len(data))\n",
    "    O = data[i].upper().replace('\\n',' ')\n",
    "    O = np.array([ord(s) for s in O])\n",
    "    O = np.maximum(O[(O>=65)*(O<90)+(O==32)]-64,0)\n",
    "    j = np.random.randint(len(O)-T)\n",
    "    return O[j:j+T]\n",
    "\n",
    "# Takes a sequence of integers between 0 and 26 (HMM representation)\n",
    "# and converts it back to a string of characters\n",
    "def tochar(O):\n",
    "    return \"\".join([\"%s\"%chr(o) for o in (O+32*(O==0)+64*(O>0.5))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train the HMM, we use a stochastic optimization algorithm where the Baum-Welch procedure is applied to randomly drawn sequences of $T=50$ characters at each iteration. The HMM has 27 visible states (A-Z + whitespace) and 200 hidden states. Because the Baum-Welch procedure optimizes for the sequence taken as input, and not necessarily the full text, it can take fairly large steps in the parameter space, which is inadequate for stochastic optimization. We consider instead for the parameters $\\lambda = (A,B,\\Pi)$ the update rule $\\lambda^{new} = (1-\\gamma) \\lambda + \\gamma \\bar \\lambda$, where $\\bar \\lambda$ contains the candidate parameters obtained from Equations 40a-c. A reasonable value for $\\gamma$ is $0.1$. Procedure:\n",
    "\n",
    "* *Create* a new class `HMMChar` that extends the class `HMM` provided in `hmm.py`.\n",
    "* *Implement* for this class a new method `HMMchar.learn(self)` that overrides the original methods, and implements the proposed update rule instead.\n",
    "* *Implement* the stochastic training procedure and run it.\n",
    "* *Monitor* $\\log p(O|\\lambda)$ on the test set at multiple iterations for sequences of same length as the one used for training. (Hint: for less noisy log-probability estimates, use several sequences or a moving average.)\n",
    "\n",
    "In order to visualize what the HMM has learned, we would like to generate random text from it. A well-trained HMM should generate character sequences that have some similarity with the text it has been trained on. Procedure:\n",
    "\n",
    "* *Implement* a method `generate(self,T)` of the class `HMMChar` that takes as argument the length of the character sequence that has to be generated.\n",
    "* *Test* the method by generating a sequence of 250 characters and comparing it with original text and a purely random sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "na = np.newaxis\n",
    "\n",
    "class hmmCHAR(hmm.HMM):\n",
    "\n",
    "    def learn(self):\n",
    "\n",
    "        # Compute gamma\n",
    "        self.gamma = self.alpha*self.beta / self.pobs\n",
    "        \n",
    "        # Compute xi and psi\n",
    "        self.xi = self.alpha[:-1,:,na]*self.A[na,:,:]*self.beta[1:,na,:]*self.Z[1:,na,:] / self.pobs\n",
    "        self.psi = self.gamma[:,:,na]*(self.O[:,na,na] == np.arange(self.B.shape[1])[na,na,:])\n",
    "        \n",
    "        # Update HMM parameters\n",
    "        self.A  = 0.9*self.A + 0.1*(self.xi.sum(axis=0)  / self.gamma[:-1].sum(axis=0)[:,na])\n",
    "        self.B  = 0.9*self.B + 0.1*(self.psi.sum(axis=0) / self.gamma.sum(axis=0)[:,na])\n",
    "        self.Pi = 0.9*self.Pi + 0.1*(self.gamma[0])\n",
    "\n",
    "    def generate(self, l):\n",
    "\n",
    "        N, M = len(self.Pi), self.B.shape[1]\n",
    "\n",
    "        s = np.random.choice(N, p=self.Pi)\n",
    "        O = []\n",
    "\n",
    "        for i in range(l):\n",
    "\n",
    "            O += [np.random.choice(M, p=self.B[s])]\n",
    "\n",
    "            s = np.random.choice(N, p=self.A[s])\n",
    "\n",
    "        return np.array(O)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it:0, logptrain:-159.76122490805164, logptest:-162.6892437882859\n",
      "it:100, logptrain:-138.44385167560276, logptest:-140.83354622965166\n",
      "it:200, logptrain:-131.01955199653761, logptest:-136.0992180339804\n",
      "it:300, logptrain:-125.83866177384347, logptest:-132.59053333212637\n",
      "it:400, logptrain:-122.42388287715173, logptest:-130.6425463768468\n",
      "it:500, logptrain:-120.04804274694736, logptest:-129.22476288781922\n",
      "it:600, logptrain:-118.10663569539727, logptest:-128.30700235442563\n",
      "it:700, logptrain:-116.94383762475259, logptest:-127.55111094532538\n",
      "it:800, logptrain:-115.96543278164522, logptest:-127.2491687625846\n",
      "it:900, logptrain:-115.1290868058525, logptest:-126.61096049206907\n",
      "it:1000, logptrain:-114.39727268370443, logptest:-126.20974318123749\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hmmchar = hmmCHAR(200,27)\n",
    "\n",
    "trainsample = lambda: sample(newsgroups_train.data)\n",
    "testsample  = lambda: sample(newsgroups_test.data)\n",
    "\n",
    "pobstrain = []\n",
    "pobstest = []\n",
    "\n",
    "for k in range(1001):\n",
    "\n",
    "    Otrain = trainsample()\n",
    "    Otest = testsample()\n",
    "\n",
    "    hmmchar.loaddata(Otrain)\n",
    "    hmmchar.forward()\n",
    "    hmmchar.backward()\n",
    "    hmmchar.learn()\n",
    "\n",
    "    hmmchar.loaddata(Otrain)\n",
    "    hmmchar.forward()\n",
    "    pobstrain += [hmmchar.pobs]\n",
    "\n",
    "    hmmchar.loaddata(Otest)\n",
    "    hmmchar.forward()\n",
    "    pobstest += [hmmchar.pobs]\n",
    "\n",
    "    if k%100 == 0:\n",
    "        print(f'it:{k}, logptrain:{np.mean(np.log(pobstrain))}, logptest:{np.mean(np.log(pobstest))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original:\n",
      " LINES  DISTRIBUTION WORLD NNTPPOSTINGHOST WMITEDU    HOW IS IT THAT PLACEBOS ARE LEGAL  IT WOULD SEEM TO ME THAT IF AS A PATIENT YOU PURCHASE A DRUG YOUVE BEEN PRESCRIBED AND ITS JUST SUGAR OR WHATEVER THERES A FEW LEGAL COMPLICATIONS THAT ARISE    \n",
      "\n",
      "learned:\n",
      " STE UNIPE  ARHITER OS O DOIRTPSWEXARS ORMAF  TERBERI DORG O QET  R EVH I  FOUN OPSTSRESSBE JIUG  AF OT PANT O WINE U PORTIIT OS RELEID NURTICDUESK PHEMEP ONDENE B ECITTE DEC A AENT TIEHABAPTE FEV AND U NIPRAGWIILCIT ISED  SIE   SD PCT NK HRUES WEM A\n",
      "\n",
      "random:\n",
      " HWFQCDRZOKTUBEPATEGKJVLTUWABXLYPWPCKAUYDCWVNQ WHYERYIUMZBVLAJIZNKFUGNEGKGYJUXRWTFUABVBHMBSMOKFSUB KDIPINLFVPAKTBIRFSRJAPAEP FXQOPUVVAWKOTMGJTBVVJUEDJAOGVDFLGTPQCFUMXHYWPOBJHTWEDLHQKYLLRNMFKGZVJGAK N ZNXDKQLGEEDOPTQRWFSKMAKRGWSDUVKLLDTEKNHKBVEKBMXA X\n"
     ]
    }
   ],
   "source": [
    "print(\"original:\\n\"+tochar(sample(newsgroups_test.data,T=250)))\n",
    "print(\"\\nlearned:\\n\"+tochar(hmmchar.generate(250)))\n",
    "print(\"\\nrandom:\\n\" +tochar(hmmCHAR(200,27).generate(250)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
